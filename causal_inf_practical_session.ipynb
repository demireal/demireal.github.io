{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The real-world dataset: NHEFS\n",
        "\n",
        "In this hands-on session, we will perform causal analyses on a real-world healthcare dataset, known as the *NHANES I Epidemiologic Follow-up Study (NHEFS)* dataset. It is a government-initiated longitudinal study designed to investigate the relationships between clinical, nutritional, and behavioral factors. For more detail, please see see the [CDC webpage](https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/).\n",
        "\n",
        "Our main task is to estimate the average effect (ATE) of quitting smoking ($T$) on weight gain ($Y$). The NHEFS cohort includes 1,566 cigarette smokers between 25 - 74 years of age who completed two medical examinations at separate time points: a baseline visit and a follow-up visit approximately 10 years later. Individuals were identified as the treatment group if they reported smoking cessation before the follow-up visit. Otherwise, they were assigned to the control group. Finally, each individual’s weight gain, $Y$, is the difference in *kg* between their body weight at the follow-up visit and their body weight at the baseline visit.\n",
        "\n",
        "# Our objectives in the session\n",
        "\n",
        "We aim to cover the following:\n",
        "1. Learn how to implement propensity score re-weighting to estimate the ATE in Python.\n",
        "2. Learn how to implement covariate adjustment strategies to estimate the conditional average treatment effect (CATE) as well as ATE in Python.\n",
        "3. By comparing *naive* estimates to what we have in (1) and (2), observe how confounders, when unadjusted, can introduce bias into the ATE estimate.\n",
        "4. Learn how to check whether some of the statistical properties required for causal inference are satisfied in the data.\n",
        "5. Learn how to implement simple and causally motivated decision-making policies on top of CATE estimations.\n",
        "\n",
        "The remainder of the notebook will guide us through this task and provide the necessary boilerplate code."
      ],
      "metadata": {
        "id": "wgA9XP6FnhOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VufvwU5ikqF6"
      },
      "outputs": [],
      "source": [
        "# import packages and load the data\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nhefs_all = pd.read_csv(\"https://github.com/mlhcmit/psets/blob/master/2025/pset3/nhefs.csv?raw=true\")\n",
        "\n",
        "outcome_col = [\"wt82_71\"]  # weight gain measured after 10 years from the baseline.\n",
        "treatment_col = [\"qsmk\"]   # indicator for smoking cessation\n",
        "\n",
        "# drop samples with missing outcomes for the assignment\n",
        "# note that this could introduce selection bias and a more principled analysis would account for the censored samples.\n",
        "missing = nhefs_all[outcome_col].isnull().any(axis=1)\n",
        "nhefs = nhefs_all.loc[~missing]"
      ],
      "metadata": {
        "id": "ABueiKIap6UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The causal problem and challenges\n",
        "\n",
        "Let $Y$ denote an outcome of interest and $T \\in \\{0, 1\\}$ denote a binary treatment. Furthermore, let $Y(t)$ denote the potential outcome of an individual under treatment, $T = t$. In randomized experiments, we learned that the average treatment effect (ATE) can be identified as,\n",
        "\n",
        "\\begin{align}\n",
        "  E[Y(1) - Y(0)] = E[Y|T = 1] - E[Y|T=0],\n",
        "\\end{align}\n",
        "\n",
        "In other words, in randomized experiments where a treatment is randomly assigned, association admits a *causal* interpretation.\n",
        "\n",
        "In observational data, however, treatment (i.e. quit smoking) is not randomly assigned, and it is very likely dependent on patient characteristics at the baseline. For example, we can reasonably assume that a proclivity for alcohol and exercise habits may influence one's decision to quit smoking (i.e. treatment) as well as weight gain (i.e. outcome). Therefore, if we fail to adjust for these factors (i.e. confounders), we may introduce bias in our estimate for the causal effect of quitting smoking on weight gain.\n",
        "\n",
        "As the lecture covered, we will need additional assumptions and adjustment strategies, which you will get a chance to utilize throughout this problem set. We will start with computing the mean difference in outcome between the treatment (quitters; $T = 1$) and control groups (non-quitters; $T = 0$), without any adjustment for potential confounders."
      ],
      "metadata": {
        "id": "yEzEtIFYrQOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #1 #############################################################\n",
        "#                                                                            #\n",
        "# Compute mean difference in outcomes between treatment (quitters) and       #\n",
        "# control groups (non-quitters). In other words, E[Y|T=1] - E[Y|T=0].        #\n",
        "#                                                                            #\n",
        "##############################################################################\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "jl5yoBsaw0pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the NHFES data\n",
        "\n",
        "We have seen one simple example of why we should adjust for confounders in order to estimate the ATE in an unbiased way. In this section, we will use the NHFES dataset to do a simple exploratory analysis on the provided set of confounders and investigate important assumptions we have to make, including overlap (i.e. common support; $0 < P(T = t|X = x) < 1,~\\forall t, x$), before adjusting for the set of confounders. Note that throughout the analysis, we'll assume ignorability (i.e. $(Y(0), Y(1)) \\perp \\! \\!\\! \\! \\perp T | X$) unless mentioned otherwise.\n",
        "\n",
        "We will provide code that generates distributions of confounders across treatment and control groups as well as a summary table, so you can assess how the treatment and control groups differ in their confounder distributions.\n",
        "\n",
        "# Provided set of confounders and explanations:\n",
        "\n",
        "1. sex - 0: MALE 1: FEMALE\n",
        "2. age - age in 1971\n",
        "3. race - 0: WHITE 1: BLACK OR OTHER IN 1971\n",
        "4. ht - HEIGHT IN CENTIMETERS IN 1971\n",
        "5. education - EDUCATION BY 1971: 1: 8TH GRADE OR LESS, 2: HS DROPOUT, 3: HS, 4:COLLEGE DROPOUT, 5: COLLEGE OR MORE\n",
        "6. alcoholpy - HAVE YOU HAD 1 DRINK PAST YEAR? IN 1971,  1:EVER, 0:NEVER; 2:MISSING\n",
        "7. smokeintensity - NUMBER OF CIGARETTES SMOKED PER DAY IN 1971\n",
        "8. smokeyrs - YEARS OF SMOKING\n",
        "9. wt71 - WEIGHT AT THE BASELINE IN 1971, IN KILOGRAMS\n",
        "10. exercise - IN RECREATION, HOW MUCH EXERCISE? IN 1971, 0:much exercise, 1:moderate exercise, 2:little or no exercise\n"
      ],
      "metadata": {
        "id": "I5OcJMa5If5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize distributions of confounders across treatment and control groups."
      ],
      "metadata": {
        "id": "I3xANaC5OSh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confounders_cols = [\n",
        "    'sex', 'age', 'race', 'ht', 'education', 'alcoholpy', 'smokeintensity', 'smokeyrs', 'wt71', 'exercise'\n",
        "]\n",
        "\n",
        "nhefs_quit = nhefs[nhefs.qsmk == 1]\n",
        "nhefs_noquit = nhefs[nhefs.qsmk == 0]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "num_bins = 20\n",
        "\n",
        "for idx, feat in enumerate(confounders_cols):\n",
        "  plt.subplot(4,3,idx + 1)\n",
        "  sns.distplot(nhefs_quit[feat], label = 'quitters', bins = num_bins)\n",
        "  sns.distplot(nhefs_noquit[feat], label = 'non-quitters', bins = num_bins)\n",
        "  plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D9_ztPer-zwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create summary table"
      ],
      "metadata": {
        "id": "4RGHefhIc9RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nhefs['university'] = (nhefs.education == 5).astype('int')\n",
        "nhefs['no_exercise'] = (nhefs.exercise == 2).astype('int')\n",
        "\n",
        "summaries = OrderedDict((\n",
        "    ('sex', lambda x: (100 * (x == 0)).mean()),\n",
        "    ('age', 'mean'),\n",
        "    ('race', lambda x: (100 * (x == 0)).mean()),\n",
        "    ('ht', 'mean'),\n",
        "    ('university', lambda x: 100 * x.mean()),\n",
        "    ('alcoholpy', lambda x: (100 * (x == 1)).mean()),\n",
        "    ('smokeintensity', 'mean'),\n",
        "    ('smokeyrs', 'mean'),\n",
        "    ('wt71', 'mean'),\n",
        "    ('no_exercise', lambda x: 100 * x.mean()),\n",
        "))\n",
        "\n",
        "table = nhefs.groupby('qsmk').agg(summaries)\n",
        "table.sort_index(ascending=False, inplace=True)\n",
        "table = table.T\n",
        "\n",
        "table.index = [\n",
        "    'Men, %',\n",
        "    'Age, years',\n",
        "    'White, %',\n",
        "    'Height at the baseline, cm',\n",
        "    'University education, %',\n",
        "    'Drinks alcohol, %',\n",
        "    'Cigarettes/day',\n",
        "    'Years smoking',\n",
        "    'Weight at the baseline, kg',\n",
        "    'Little or no exercise, %',\n",
        "]\n",
        "\n",
        "# add confidence interval\n",
        "table_with_ci = pd.DataFrame([], index = table.index, columns = ['treatment (qsmk = 1)', 'control (qsmk = 0)'])\n",
        "\n",
        "for idx, col_name in zip(table.index, confounders_cols):\n",
        "  if '%' in idx:\n",
        "    # treat group\n",
        "    p_hat = table.at[idx, 1]/100\n",
        "    se_treat = math.sqrt(p_hat*(1-p_hat)/len(nhefs_quit))*100\n",
        "    lower_95_ci = table.at[idx, 1] - 1.96*se_treat; upper_95_ci = table.at[idx, 1] + 1.96*se_treat;\n",
        "    table_with_ci.at[idx, 'treatment (qsmk = 1)'] = '{0:>0.1f} - {1:>0.1f}'.format(lower_95_ci, upper_95_ci)\n",
        "\n",
        "    # control group\n",
        "    p_hat = table.at[idx, 0]/100\n",
        "    se_control = math.sqrt(p_hat*(1-p_hat)/len(nhefs_noquit))*100\n",
        "    lower_95_ci = table.at[idx, 0] - 1.96*se_treat; upper_95_ci = table.at[idx, 0] + 1.96*se_treat;\n",
        "    table_with_ci.at[idx, 'control (qsmk = 0)'] = '{0:>0.1f} - {1:>0.1f}'.format(lower_95_ci, upper_95_ci)\n",
        "\n",
        "  else:\n",
        "    # treat group\n",
        "    std = nhefs_quit[col_name].std()\n",
        "    se_treat = std/math.sqrt(len(nhefs_quit))\n",
        "    lower_95_ci = table.at[idx, 1] - 1.96*se_treat; upper_95_ci = table.at[idx, 1] + 1.96*se_treat;\n",
        "    table_with_ci.at[idx, 'treatment (qsmk = 1)'] = '{0:>0.1f} - {1:>0.1f}'.format(lower_95_ci, upper_95_ci)\n",
        "\n",
        "    # control group\n",
        "    std = nhefs_noquit[col_name].std()\n",
        "    se_control = std/math.sqrt(len(nhefs_noquit))\n",
        "    lower_95_ci = table.at[idx, 0] - 1.96*se_treat; upper_95_ci = table.at[idx, 0] + 1.96*se_treat;\n",
        "    table_with_ci.at[idx, 'control (qsmk = 0)'] = '{0:>0.1f} - {1:>0.1f}'.format(lower_95_ci, upper_95_ci)\n",
        "\n",
        "print('95% confidence intervals are shown')\n",
        "table_with_ci"
      ],
      "metadata": {
        "id": "9AOElqJDy7K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions :\n",
        "\n",
        "1. Does the provided set of confounders make sense? Pick a couple confounders and discuss why they qualify as confounders.\n",
        "\n",
        "2. Based on the provided plots and table, can you conclude the overlap assumption is satisfied? Why or why not? What would be an implication of a high-dimensional set of confounders on the overlap assumption?\n"
      ],
      "metadata": {
        "id": "GGBGFrBsBzgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Propensity Score Re-weighting\n",
        "\n",
        "We have seen how failing to adjust for the set of confounders can introduce bias in our causal estimate. As we learned in class, propensity score reweighting is one of the most widely used adjustment methods for causal inference. Given that the ignorability and overlap assumptions are satisfied and that our propensity prediction model is correctly specified, scaling each outcome by the inverse of the corresponding propensity score creates a pseudo-population where the treatment assignment is effectively random. More formally, our estimated ATE, $\\hat{\\theta}$, is derived as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\theta} = \\frac{1}{N}\\sum_{i ~\\text{s.t.}~ t_i = 1}\\frac{y_i}{\\hat{P}(T = 1|X = x_i)} - \\frac{1}{N}\\sum_{i ~\\text{s.t.}~ t_i = 0}\\frac{y_i}{\\hat{P}(T = 0|X = x_i)}\n",
        "\\end{align}\n",
        "\n",
        "In this part, you will use sklearn's [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to estimate the probability of treatment given the set of confounders (i.e. $\\hat{P}(T = t|X)$). Note that CV stands for \"cross-validation\" and it automatically handles choosing the best regularization parameter when fitting the Logistic Regression model.\n",
        "\n",
        "As an aside, you would actually want to do *cross-fitting* to avoid bias due to overfitting and estimate propensity scores. If you are interested in learning more, please see [here](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2021/03/ciwhatif_hernanrobins_30mar21.pdf) (particularly Chapter 18-4)."
      ],
      "metadata": {
        "id": "AuoT1ZknLm4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #2 #############################################################\n",
        "#                                                                            #\n",
        "# Estimate propensity scores for both treatment and control groups. For the  #\n",
        "# LogisticRegression hyperparameters, Use LogisticRegressionCV from sklearn  #\n",
        "# to choose the best L1 penalty coefficient.  Use \"fit\" and \"predict_proba\"  #\n",
        "# functions to fit your model and use it to predict propensity scores.       #\n",
        "#                                                                            #\n",
        "##############################################################################\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "ULb3_yt4B1W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions :\n",
        "3. Obtain the coefficients for confounders in your propensity estimation model (use `model.coef_` to obtain coefficients from the trained model). Do the coefficients make sense? Choose coefficients for a couple of confounders and discuss."
      ],
      "metadata": {
        "id": "sL6p7tPhLMpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have estimated propensity scores, we can visualize the empirical distribution of our estimated propensity scores and gain additional insights on the validity of the overlap condition. We have already provided the code for the visualization, and all you have to do is to assign your estimated propensity scores to the `propensity_scores` variable.\n"
      ],
      "metadata": {
        "id": "VLkmPcuEpTWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(propensity_scores, bins = num_bins)\n",
        "plt.legend()\n",
        "plt.xlabel('propensity scores')\n",
        "plt.figure(figsize=(10,5))"
      ],
      "metadata": {
        "id": "eN1SeztSk3LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions :\n",
        "4. Let us assume that our propensity score is correctly specified, what does the above plot tell you about whether or not the overlap assumption is met?\n",
        "5. What advantage does this approach offer over comparing the marginal distributions of the confounders?"
      ],
      "metadata": {
        "id": "Vv9p4A9_FVwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to use the propensity re-weighting method to estimate average treatment effect (ATE), $\\hat{\\theta}$, using the formula provided above. We restate it below.\n",
        "\n",
        "\\begin{align}\n",
        "\\widehat{\\text{ATE}} = \\frac{1}{N}\\sum_{i ~\\text{s.t.}~ t_i = 1}\\frac{y_i}{\\hat{P}(T = 1|X = x_i)} - \\frac{1}{N}\\sum_{i ~\\text{s.t.}~ t_i = 0}\\frac{y_i}{\\hat{P}(T = 0|X = x_i)}\n",
        "\\end{align}\n",
        "\n",
        "where $N$ is the size of the entire cohort."
      ],
      "metadata": {
        "id": "Uder_SzYHnrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #3 #############################################################\n",
        "#                                                                            #\n",
        "# Apply propensity re-weighting using the provided formula above and         #\n",
        "# estimate ATE.                                                              #\n",
        "#                                                                            #\n",
        "##############################################################################\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "Jwqnj8VhHizO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Covariate Adjustment\n",
        "\n",
        "Covariate adjustment is another popular strategy where one explictly models the relationships between the outcome $Y$, treatment $T$ and a set of confounders $X$. With the ignorability and overlap assumptions met, a correctly specified model provides an unbiased estimate of the average treatment effect (ATE). In this section, you will use sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model to obtain\n",
        "\\begin{align}\n",
        "  \\hat{E}[Y|T = t, X = x].\n",
        "\\end{align}\n",
        "\n",
        "As we learned in class, when the identifiability assumptions are met, we can relate the data to the potential outcomes. Namely, $\\hat{E}[Y(t)|X = x] = \\hat{E}[Y|T = t, X = x]$. Therefore, we have\n",
        "\\begin{align}\n",
        "  \\widehat{\\text{CATE}} &= \\hat{E}[Y(1)|X = x] - \\hat{E}[Y(0)|X = x] \\\\\n",
        "  &= \\hat{E}[Y|T = 1, X = x] - \\hat{E}[Y|T = 0, X = x] \\\\\n",
        "\\end{align}\n",
        "Then, $\\widehat{\\text{ATE}}$ can be estimated as\n",
        "\\begin{align}\n",
        "   \\hat{\\theta} &= \\hat{E}_{x \\sim p(x)}[\\widehat{\\text{CATE}}] \\\\\n",
        "  &= \\frac{1}{N} \\sum_{i}^N \\hat{E}[Y|T = 1, X = x_i] - \\hat{E}[Y|T = 0, X = x_i],\n",
        "\\end{align} where $N$ is the size of the entire cohort."
      ],
      "metadata": {
        "id": "sM75BswNl0RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #4 #############################################################\n",
        "#                                                                            #\n",
        "# Use covariate adjustment and estimate the ATE.                             #\n",
        "# Use sklearn's LinearRegression package. Use \"fit\" function.                #\n",
        "#                                                                            #\n",
        "##############################################################################\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "DjY2MCX-qbgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions :\n",
        "6. In the linear model, the coefficient for the treatment variable $T$ corresponds to $\\hat{\\theta}$ (the ATE estimate). Can you explain why?\n",
        "\n",
        "7. Report your estimated ATEs from the propensity score re-weighting and covariate adjustment methods. How are they different from the mean difference of outcomes between the treatment and control groups without adjusting for confounders (from task #1)? If there is a difference, does its sign make sense (i.e., whether the naive estimate is an under or over estimation)?\n",
        "\n",
        "8. Do you think our model for the covariate adjustment task (i.e. linear regression) is valid? What are some potential limitations of this linear model approach?"
      ],
      "metadata": {
        "id": "bSNXiHqUwwuf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9onz8KL2GOyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clinical Decision-Making on the Basis of Conditional (Individualized) Average Treatment Effects (CATE)\n",
        "\n",
        "In practice, we would like to make clinical decision not on the basis of the ATE, but on the CATE. In the next synthetic toy example, we will see how such a decision-making rule can be implemented on the basis of CATE.\n",
        "\n",
        "## Synthetic Data Generation Process\n",
        "\n",
        "## Overview\n",
        "We will create a realistic clinical dataset where patients choose between two blood pressure medications. The data generation process mimics real-world clinical scenarios with confounding, complex treatment effects, and realistic patient characteristics.\n",
        "\n",
        "---\n",
        "\n",
        "## Patient Characteristics (Confounders)\n",
        "\n",
        "### Baseline Variables\n",
        "We generate 5 key patient characteristics that influence both treatment selection and outcomes:\n",
        "\n",
        "| Variable | Distribution | Range | Clinical Meaning |\n",
        "|----------|-------------|-------|------------------|\n",
        "| **Age** | Normal(65, 15) | 30-90 years | Patient age |\n",
        "| **Baseline BP** | Normal(160, 20) | 120-220 mmHg | Systolic blood pressure before treatment |\n",
        "| **BMI** | Normal(28, 5) | 18-45 kg/m² | Body mass index |\n",
        "| **Diabetes** | Bernoulli(0.3) | 0/1 | Diabetes status (30% have diabetes) |\n",
        "| **Kidney Function** | Normal(60, 20) | 15-120 mL/min | Estimated glomerular filtration rate (eGFR) |\n"
      ],
      "metadata": {
        "id": "BPzIRv8u0h1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.tree import export_text\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def generate_synthetic_data(n=2000):\n",
        "    ## TODO: Task #1 ###################################################################\n",
        "    #                                                                                  #\n",
        "    # Generate synthetic patient cohort following the distributions in the table above #\n",
        "    # Use numpy                                                                        #\n",
        "    #                                                                                  #\n",
        "    ####################################################################################\n",
        "\n",
        "    ##########################\n",
        "    #                        #\n",
        "    #        SOLUTION        #\n",
        "    #                        #\n",
        "    ##########################\n",
        "\n",
        "    age = ...\n",
        "    age = np.clip(age, 30, 90)\n",
        "\n",
        "    baseline_bp = ...\n",
        "\n",
        "    bmi = ...\n",
        "\n",
        "    kidney_function = ...\n",
        "\n",
        "    diabetes = ...\n",
        "\n",
        "    # Create DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'age': age,\n",
        "        'baseline_bp': baseline_bp,\n",
        "        'bmi': bmi,\n",
        "        'diabetes': diabetes,\n",
        "        'kidney_function': kidney_function\n",
        "    })\n",
        "\n",
        "    # Save normalized versions\n",
        "    data['age_norm'] = (data['age'] - 65) / 15\n",
        "    data['bp_norm'] = (data['baseline_bp'] - 160) / 20\n",
        "    data['bmi_norm'] = (data['bmi'] - 28) / 5\n",
        "    data['kidney_norm'] = (data['kidney_function'] - 60) / 20\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "2lNLzzWI33NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A ground-truth CATE function and treatment assignment strategy\n",
        "\n",
        "Below, we define a \"complex\" ground-truth CATE function and treatment assignment strategy, that depend on all the confounders generated above.\n",
        "\n",
        "Please take a moment to investigate these functions to understand how different confouders affect the effectiveness of the treatment *and* the probability of treatment assignment by the clinician."
      ],
      "metadata": {
        "id": "5sSF_dRo384U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def true_cate_function(data):\n",
        "    \"\"\"\n",
        "    Define the ground truth CATE function.\n",
        "    This represents the true difference in treatment effect between new and standard medication.\n",
        "    \"\"\"\n",
        "\n",
        "    age_norm = data['age_norm']\n",
        "    bp_norm = data['bp_norm']\n",
        "    bmi_norm = data['bmi_norm']\n",
        "    diabetes = data['diabetes']\n",
        "    kidney_norm = data['kidney_norm']\n",
        "\n",
        "    # Complex CATE function with interactions and non-linearities\n",
        "    cate = (\n",
        "        # Base effect: new drug works better for older patients with higher BP\n",
        "        5 * age_norm * bp_norm +\n",
        "\n",
        "        # BMI interaction: new drug less effective for very high BMI\n",
        "        -3 * np.maximum(0, bmi_norm - 1)**2 +\n",
        "\n",
        "        # Diabetes interaction: new drug much better for diabetics with kidney issues\n",
        "        8 * diabetes * np.maximum(0, -kidney_norm) +\n",
        "\n",
        "        # Age-kidney interaction: new drug worse for elderly with good kidney function\n",
        "        -4 * np.maximum(0, age_norm) * np.maximum(0, kidney_norm) +\n",
        "\n",
        "        # Non-linear baseline BP effect\n",
        "        3 * np.sin(bp_norm * np.pi) +\n",
        "\n",
        "        # Threshold effect for very high baseline BP\n",
        "        6 * (bp_norm > 1.5).astype(int) +\n",
        "\n",
        "        # Random noise to make it more realistic\n",
        "        np.random.normal(0, 1, len(data))\n",
        "    )\n",
        "\n",
        "    return cate\n",
        "\n",
        "\n",
        "def generate_treatment_assignment(data, cate):\n",
        "    \"\"\"\n",
        "    Generate treatment assignment with confounding.\n",
        "    We simulate a case where doctors tend to give new medication to patients in more adversarial conditions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Propensity (i.e., treatment assignment score)\n",
        "\n",
        "    propensity_logit = (\n",
        "        0.5 * data['bp_norm'] +  # Higher BP more likely to get new drug\n",
        "        0.3 * data['diabetes'] +  # Diabetics more likely to get new drug\n",
        "        -0.2 * data['age_norm'] +  # Slightly less likely for very elderly\n",
        "        0.1 * data['bmi_norm']  # Higher BMI more likely to get new drug\n",
        "    )\n",
        "\n",
        "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
        "    treatment = np.random.binomial(1, propensity, len(data))\n",
        "\n",
        "    return treatment, propensity\n",
        "\n",
        "\n",
        "def generate_outcomes(data, treatment, cate):\n",
        "    \"\"\"\n",
        "    Generate potential outcomes and observed outcomes.\n",
        "    \"\"\"\n",
        "    # Potential outcome for control group (standard medication)\n",
        "    y0_base = (\n",
        "        5 +  # Base reduction in the Systolic Blood Pressure (the outcome of interest)\n",
        "        np.random.normal(0, 1, len(data))  # Random noise\n",
        "    )\n",
        "\n",
        "    # Ground-truth potential outcomes\n",
        "    y0 = y0_base         # Standard medication\n",
        "    y1 = y0_base + cate  # New medication = Standard medication + CATE\n",
        "\n",
        "    # Observed outcome\n",
        "    y_observed = treatment * y1 + (1 - treatment) * y0\n",
        "\n",
        "    return y0, y1, y_observed"
      ],
      "metadata": {
        "id": "z00MNRs35CjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate synthetic data\n",
        "\n",
        "Now we have defined all our confounding variables, and how the potential outcomes, CATE, and the treatment assignment depend on them, we go ahead and sample a cohort."
      ],
      "metadata": {
        "id": "SyOHbPzq5DE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the synthetic dataset\n",
        "print(\"Generating synthetic clinical data...\")\n",
        "data = generate_synthetic_data(n=2000)\n",
        "\n",
        "# Generate true CATE\n",
        "true_cate = true_cate_function(data)\n",
        "data['true_cate'] = true_cate\n",
        "\n",
        "# Generate treatment assignment\n",
        "treatment, propensity = generate_treatment_assignment(data, true_cate)\n",
        "data['treatment'] = treatment\n",
        "data['propensity'] = propensity\n",
        "\n",
        "# Generate outcomes\n",
        "y0, y1, y_observed = generate_outcomes(data, treatment, true_cate)\n",
        "data['y0'] = y0  # Potential outcome under control\n",
        "data['y1'] = y1  # Potential outcome under treatment\n",
        "data['outcome'] = y_observed  # Observed outcome\n",
        "\n",
        "print(f\"Synthetic dataset created with {len(data)} patients\")"
      ],
      "metadata": {
        "id": "_ol9x_478Qg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Treatment Decisions\n",
        "\n",
        "For simplicity, we assume that a higher outcome (reduction in systolic blood pressure) is always better.\n",
        "\n",
        "Therefore, when CATE is greater than zero, we would like to assign that patient to the new medicine (treatment). When the CATE is smaller than zero we would assign the patient to the standard medicine (control)."
      ],
      "metadata": {
        "id": "kK4hZYJh8TMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #2 ###################################################################\n",
        "#                                                                                  #\n",
        "# Create a column for the optimal treatment decisions for each patient             #\n",
        "# This should be based on the value of \"true_cate\"                                 #\n",
        "#                                                                                  #\n",
        "####################################################################################\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "data['optimal_treatment'] = ..."
      ],
      "metadata": {
        "id": "eo632paL81Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating the CATE via covariate adjustment\n",
        "\n",
        "We will first split the data into two equally sized chunks, and call them \"train\" and \"test\" splits out of convenience.\n",
        "\n",
        "We will use the \"train\" split to fit outcome models to estimate the CATE function (covariate adjustment).\n",
        "\n",
        "We will use the [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) from sklearn to fit these outcome models functions."
      ],
      "metadata": {
        "id": "mtFquGsK-9BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "features = ['age', 'baseline_bp', 'bmi', 'diabetes', 'kidney_function']\n",
        "X = data[features]\n",
        "y = data['outcome']\n",
        "treatment = data['treatment']\n",
        "\n",
        "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(\n",
        "    X, y, treatment, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Get corresponding true CATE and optimal treatment decision for test set\n",
        "true_cate_test = data.loc[X_test.index, 'true_cate']\n",
        "optimal_treatment_test = data.loc[X_test.index, 'optimal_treatment']\n",
        "\n",
        "## TODO: Task #3 ###################################################################\n",
        "#                                                                                  #\n",
        "# Fit separate outcome models for the control and the treatment groups.            #\n",
        "# Use the train split                                                              #\n",
        "#                                                                                  #\n",
        "####################################################################################\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "control_mask = t_train == 0    # indices of the control group (standard medicine) in the training set\n",
        "treatment_mask = t_train == 1  # indices of the treatment group (new medicine) in the training set\n",
        "\n",
        "\n",
        "model_control = ...  # initiate the RandomForestRegressor model for the control group\n",
        "...  # fit the model\n",
        "\n",
        "model_treatment = ...  # initiate the RandomForestRegressor model for the treatment group\n",
        "...  # fit the model"
      ],
      "metadata": {
        "id": "Ojq67mSoA_5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATE-based treatment decisions\n",
        "\n",
        "For simplicity, we assume that a higher reduction in the sytolic blood pressure is always better, and we want to prefer the treatment that reduces it more.\n",
        "\n",
        "Since in this example, we only care about the \"sign\" of the CATE function, we will make our treatment decisions based on the sign of the CATE function solely.\n",
        "\n",
        "A patient will get the new medicine (treatment=1) if their estimated CATE is greater than zero, and the standard medicine (treatment=0) otherwise."
      ],
      "metadata": {
        "id": "eQcDN4fCPow_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #4 ########################################################################\n",
        "#                                                                                       #\n",
        "# Estimate CATE in \"train\" split using the difference in predictions of outcome models  #\n",
        "# Make treatment decisions based on the sign of the CATE                                #\n",
        "#                                                                                       #\n",
        "#########################################################################################\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "# Predict outcomes under both treatments for test set (use X_train)\n",
        "y0_pred_train = ...\n",
        "y1_pred_train = ...\n",
        "\n",
        "# Estimate CATE in the train split\n",
        "cate_estimated_train = ...\n",
        "cate_treatment_recommendation_train = ...\n",
        "\n",
        "# repeat the steps above for the test-split for evaluation later\n",
        "\n",
        "# Predict outcomes under both treatments for test set (use X_test)\n",
        "y0_pred_test = ...\n",
        "y1_pred_test = ...\n",
        "\n",
        "# Estimate CATE in the test split\n",
        "cate_estimated_test = ...\n",
        "cate_treatment_recommendation_test = ..."
      ],
      "metadata": {
        "id": "zNXldj8ePo7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit a decision-tree in the test set based on CATE estimates\n",
        "\n",
        "CATE functions in reality can be complex. In clinical decision-making, having simpler models that can be interpreted easily have advantages.\n",
        "\n",
        "In the previous parts, we fitted two outcome models on the \"train\" split and used them to estimate the CATE for individuals in the test split.\n",
        "\n",
        "In this step, we will fit a [DecisionTree](https://scikit-learn.org/stable/modules/tree.html) on the \"train\" split to predict the treatment assignments made based on the CATE function, which could result in a simpler model."
      ],
      "metadata": {
        "id": "Wwg_lR8VBBAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Task #5 #####################################################################\n",
        "#                                                                                    #\n",
        "# Fit a decision tree in the train split to predict CATE-based treatment assignments #\n",
        "# Set max_depth=3 for easier interpretation                                          #\n",
        "# Note that there is a tradeoff with performance when we use lower depth.            #\n",
        "#                                                                                    #\n",
        "# Get treatment assignment predictions from the tree once fitted in the \"test\" split #\n",
        "#                                                                                    #\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "##########################\n",
        "#                        #\n",
        "#        SOLUTION        #\n",
        "#                        #\n",
        "##########################\n",
        "\n",
        "decision_tree = ...  # instantiate the DecisionTree\n",
        "... # fit the model\n",
        "\n",
        "tree_predictions_test = ...  # CATE-sign predictions from the tree"
      ],
      "metadata": {
        "id": "OP9ni_H4qEHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing the quality of decisions made by the tree based system and visualizing our tree-based rule\n",
        "\n",
        "Compare the performance of the tree-based strategy to that of optimal strategy and CATE-based strategy.\n",
        "\n",
        "## Questions\n",
        "\n",
        "1. Do these findings make sense (i.e., the relative ranking of the performance of different strategies)?\n",
        "2. What would it take the tree-based strategy's performance to match that of CATE-based strategy?\n",
        "3. What would it take the CATE-based strategy's performance to match that of optimal strategy?"
      ],
      "metadata": {
        "id": "WL9-0z-ZGANN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTree-based recommendation performance per treatment class:\")\n",
        "print(classification_report(optimal_treatment_test, tree_predictions_test,\n",
        "                          target_names=['Standard Med', 'New Med']))\n",
        "\n",
        "# Compare different strategies\n",
        "strategies = {\n",
        "    'Always Standard': np.zeros(len(X_test)),\n",
        "    'Always New': np.ones(len(X_test)),\n",
        "    'Random': np.random.binomial(1, 0.5, len(X_test)),\n",
        "    'CATE-based': cate_treatment_recommendation_test,\n",
        "    'Decision Tree': tree_predictions_test,\n",
        "    'True Optimal': optimal_treatment_test\n",
        "}\n",
        "\n",
        "results = {}\n",
        "y0_test = data.loc[X_test.index, 'y0']\n",
        "y1_test = data.loc[X_test.index, 'y1']\n",
        "\n",
        "for strategy_name, recommendations in strategies.items():\n",
        "    # Calculate expected outcome under this strategy\n",
        "    outcomes = recommendations * y1_test + (1 - recommendations) * y0_test\n",
        "    avg_outcome = np.mean(outcomes)\n",
        "\n",
        "    # Calculate how often we make the right decision\n",
        "    correct_decisions = np.mean(recommendations == optimal_treatment_test)\n",
        "\n",
        "    results[strategy_name] = {\n",
        "        'avg_outcome': avg_outcome,\n",
        "        'correct_decisions': correct_decisions\n",
        "    }\n",
        "\n",
        "print(\"\\nStrategy Comparison:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Strategy':<15} {'Avg Outcome':<12} {'Correct %':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for strategy, metrics in results.items():\n",
        "    print(f\"{strategy:<15} {metrics['avg_outcome']:<12.2f} {metrics['correct_decisions']:<10.1%}\")\n",
        "\n",
        "\n",
        "strategy_names = list(results.keys())\n",
        "avg_outcomes = [results[s]['avg_outcome'] for s in strategy_names]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.bar(range(len(strategy_names)), avg_outcomes, alpha=0.8)\n",
        "plt.xlabel('Strategy')\n",
        "plt.ylabel('Average Outcome (BP Reduction)')\n",
        "plt.title('Treatment Strategy Comparison')\n",
        "plt.xticks(range(len(strategy_names)),strategy_names, rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the decision tree rules\n",
        "print(\"\\nDecision Tree Rules for Clinical Use:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tree_rules = export_text(decision_tree, feature_names=features, max_depth=4)\n",
        "print(tree_rules)"
      ],
      "metadata": {
        "id": "tGFwRnlYF864"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9s_bRtCGGFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}